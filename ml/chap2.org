* 公式
- An equation for the error in the output layer :: δ(l) = dC/da(l) * σ'(z(l))  --- BP1
- An equation for the error δ(l) in terms of the error in the next layer :: δ(l) = δ(l+1) * w(l+1) * σ'(z(l)) --- BP2
- An equation for the rate of change of the cost with respect to any bias in the network :: dC/db = δ --- BP3 
- An equation for the rate of change of the cost with respect to any weight in the network :: dC/dw = a[in]δ[out]
	 
* 公式推导

δ(l) = dC/dz(l)
a(l) = σ(z(l))
z(l) = w(l) * a(l-1) + b(l)

δ(l) = dC/da(l) * da(l) / dz(l) = dC/da(l) * σ'(z(l))  --- BP1

z(l+1) = w(l+1) * σ(z(l)) + b(l+1)

δ(l) = dC/dz(l) = dC/dz(l+1) * dz(l+1) / dz(l) =>
δ(l) = δ(l+1) * dz(l+1) / dz(l)
    = δ(l+1) * d(w(l+1) * σ(z(l)) + b(l+1)) / dz(l)
    = δ(l+1) * w(l+1) * σ'(z(l)) =>
    
δ(l) = δ(l+1) * w(l+1) * σ'(z(l)) --- BP2

δ(l) = C/dz(l) = dC/db(l) * db(l)/dz(l) = dC/db(l) --- BP3

δ(l) = dC/dw(l) * dw(l)/dz(l) = dC/dw(l) * d((z(l)-b(l)) / a(l-1))/dz(l)
    = dC/dw(l) * (1/a(l-1)) =>
dC/dw(l) = δ(l) * a(l-1) --- BP4 
* Backpropagation algorithm
- Input x :: Set the corresponding activation a(1) for the input layer.
- Feedforward :: For each l = 2,3,...,L compute z(l) = w(l)a(l-1) + b(l) and a(l) = σ(z(l)).
- Output error δ(L) :: Compute the vector δ(L) = dC/da * σ'(z(L)).
- Backpropagation the error :: For each l = L-1, L-2, ...,2 compute δ(l) = ((w(l+1)δ(l+1)) * σ(z(l)).
- Output :: The gradient of the cost function is given by dC/dw(l) = a(l-1)δ(l) and dC/db(l) = δ(l).
